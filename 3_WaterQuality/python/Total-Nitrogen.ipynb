{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Water Quality Monitoring - Exploratory Data Analysis\n",
    "```L. Patterson & John Fay\n",
    "Spring 2018```\n",
    "\n",
    "In this notebook, we examine the two datsets generated from our query of the [Water Quality Data portal](https://www.waterqualitydata.us/portal/) for nutrient data in HUC 03030002 from 1970 trough 2017. The first dataset ([sites.csv](https://www.waterqualitydata.us/Station/search?countrycode=US&statecode=US%3A37&siteType=Lake%2C%20Reservoir%2C%20Impoundment&siteType=Stream&huc=03030002&sampleMedia=Water&characteristicType=Nutrient&minresults=500&startDateLo=01-01-1970&startDateHi=12-31-2017&mimeType=csv&zip=yes&sorted=no)) contains data on the locations where samples were collected, and the second dataset ([results.csv](https://www.waterqualitydata.us/Result/search?countrycode=US&statecode=US%3A37&siteType=Lake%2C%20Reservoir%2C%20Impoundment&siteType=Stream&huc=03030002&sampleMedia=Water&characteristicType=Nutrient&minresults=500&startDateLo=01-01-1970&startDateHi=12-31-2017&mimeType=csv&zip=yes&sorted=no)) contains data on the nutrient samples collected at these sites. \n",
    "\n",
    "Our goals here are to upload, prepare (tidy, explore, and merge) these data, and ultimately provide some visualizations that reveal the state of water quality in Jordan lake. We'll begin with the **sites** dataset, examining a few attributes about these data, and the move to the **results** where we filter our data for 'clean' records and tidy it for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep the workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import modules\n",
    "import pandas as pd\n",
    "import folium\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#Enable notebook plotting\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and analyze the water quality *sites* data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data in as a Pandas dataframe\n",
    "We'll first load in the sites data into a dataframe, doing a few checks to ensure the data import ok and get a feeling for what we're looking at. The data are located in the data folder as the file `station.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the data into the 'sites' dataframe \n",
    "sites = pd.read_csv('../data/station.csv',\n",
    "                   dtype={'HUCEightDigitCode':'str'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the dimensions of the data frame\n",
    "sites.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the columns and their data types\n",
    "sites.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the data: Plot the sites on a map\n",
    "Plot a map to see where water quality data were present from this portal.<br> First we'll plot all the points, then we'll remove sites with small drainage areas and see how many remain..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot all sites on a map\n",
    "\n",
    "#Find center coordinates from medians of lat and long columns\n",
    "medLat = sites['LatitudeMeasure'].median()\n",
    "medLng = sites['LongitudeMeasure'].median()\n",
    "\n",
    "#Create the initial map\n",
    "m = folium.Map(location=[medLat,medLng],\n",
    "               zoom_start=9,\n",
    "               tiles='stamenterrain')\n",
    "\n",
    "#Loop through all features and add them to the map as markers\n",
    "for row in sites.itertuples():\n",
    "    #Get info for the record\n",
    "    lat = row.LatitudeMeasure\n",
    "    lng = row.LongitudeMeasure\n",
    "    name = row.MonitoringLocationName\n",
    "    #Create the marker object, adding them to the map object\n",
    "    folium.CircleMarker(location=[lat,lng],\n",
    "                        popup=name,\n",
    "                        color='red',\n",
    "                        fill=True,\n",
    "                        fill_opacity=0.6,\n",
    "                        radius=3,\n",
    "                        stroke=False).add_to(m)\n",
    "    \n",
    "#Show map\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot sites with an area > 25 sq mi\n",
    "sites2 = sites[sites['DrainageAreaMeasure/MeasureValue'] > 25]\n",
    "sites2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Only 10 records!* This seems small. Let's plot these in blue on top of the map created above to see where these 10 sites occur. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop through all features and add them to the map as markers\n",
    "for row in sites2.itertuples():\n",
    "    #Get info for the record\n",
    "    lat = row.LatitudeMeasure\n",
    "    lng = row.LongitudeMeasure\n",
    "    name = row.MonitoringLocationName\n",
    "    #Create the marker object, adding them to the map object\n",
    "    folium.CircleMarker(location=[lat,lng],\n",
    "                        popup=name,\n",
    "                        color='blue',\n",
    "                        fill=True,\n",
    "                        fill_opacity=0.8,\n",
    "                        radius=3,\n",
    "                        stroke=False).add_to(m)\n",
    "    \n",
    "#Show map\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take a closer look at the drainage measurement values and see there are numerous `NA` values. *Clearly, filtering by drainage area will not work*. The main point I want to make here is <mark>**don’t be afraid to play with the data.**</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s replot the site map but now change the popup name to see which site names are located within each branch of Jordan Lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show the map\n",
    "m = folium.Map(location=[medLat,medLng],\n",
    "               zoom_start=9,\n",
    "               tiles='stamenterrain')\n",
    "\n",
    "#Loop through all features and add them to the map as markers\n",
    "for row in sites.itertuples():\n",
    "    #Get info for the record\n",
    "    lat = row.LatitudeMeasure\n",
    "    lng = row.LongitudeMeasure\n",
    "    name = row.MonitoringLocationIdentifier #<-- Change the field to show as the popup\n",
    "    #Create the marker object, adding them to the map object\n",
    "    folium.CircleMarker(location=[lat,lng],\n",
    "                        popup=name,\n",
    "                        color='red',\n",
    "                        fill=True,\n",
    "                        fill_opacity=0.6,\n",
    "                        radius=3,\n",
    "                        stroke=False).add_to(m)\n",
    "    \n",
    "#Show map\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's cull some unused columns from our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keepColumns = [0,2,3,7,8,11,12]\n",
    "sites = sites.iloc[:,keepColumns]\n",
    "sites.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Import and explore the water quality *measurement* data\n",
    "The second dataset downloaded from the [Water Quality Data portal](https://www.waterqualitydata.us/portal/) were the water quality results, stored in the [`result.csv`](https://www.waterqualitydata.us/Result/search?countrycode=US&statecode=US%3A37&siteType=Lake%2C%20Reservoir%2C%20Impoundment&siteType=Stream&huc=03030002&sampleMedia=Water&characteristicType=Nutrient&minresults=500&startDateLo=01-01-1970&startDateHi=12-31-2017&mimeType=csv&zip=yes&sorted=no) file. Let's now import this \\[large\\] file and tidy it up for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the results.csv\n",
    "results = pd.read_csv('../data/result.csv',\n",
    "                      low_memory=False      #This is required as it's a large file...\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the dimensions of the dataframe: it's BIG with close to 100k records!\n",
    "results.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the data by filtering records to only the ones we want\n",
    "Once we load in the measurements data, we need to do some cleaning. The first step is to filter the data to what we want to use. For example,\n",
    "* We want to restrict our analysis to streams, filtering out all other media.<br><br>\n",
    "\n",
    "* We want to make sure we are using routine samples and not extreme event sampling (biased for specific occasions and not for estimating annual average load). So, we'd like to filter out events like storms, droughts, floods, and spring break ups from our analysis.<br><br>\n",
    "\n",
    "* We determine what type of Nitrogen we want to use. From the literature we found that regulations for Nitrogen include: nitrate, nitrite, ammonia and organic forms. Doing some reading about the WQX standards, you learn that Nitrogen, mixed froms incorporates all of the above forms of nitrogen.<br><br>\n",
    "\n",
    "*  We also want to make sure we are looking at *total nitrogen*, so we want to make sure the `Results Sample Fraction Text` only includes those with `Total`.\n",
    "\n",
    "It's handy to know the to what each field in our data refers and what it's values are. An easy way to inspect the latter is with the Pandas `unique()` function which lists the unique values found in a give column. **Or**,, better yet, the `value_counts()` function with lists each unique value along with the number of records having that value. ***Or***, better still, we could *plot* those value_count() values..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List the unique values inthe HydrologicEvent column\n",
    "results['HydrologicEvent'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List the unique values inthe HydrologicEvent column AND COUNT THE NUMBER OF RECORDS IN EACH\n",
    "results['HydrologicEvent'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List the unique values inthe CharacteristicName column\n",
    "results['CharacteristicName'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering in Pandas can be done by creating bitwise (i.e. True/False) \"mask\" and then combining them with logical operations.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1 in filtering: creating a mask for each criteria\n",
    "mediaMask = results['ActivityMediaSubdivisionName'] == \"Surface Water\"\n",
    "\n",
    "hydroMask = ~results['HydrologicEvent'].isin((\"Storm\",\"Flood\",\"Spring breakup\",\"Drought\")) \n",
    "\n",
    "charMask = results['CharacteristicName'] == \"Nitrogen, mixed forms (NH3), (NH4), organic, (NO2) and (NO3)\"\n",
    "\n",
    "sampFracMask = results['ResultSampleFractionText'] == 'Total'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2 in filtering: Applying the masks using logical combinations\n",
    "nitrogen = results[mediaMask & hydroMask & charMask & sampFracMask] \n",
    "nitrogen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check that the filters worked\n",
    "nitrogen['ActivityMediaSubdivisionName'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nitrogen['HydrologicEvent'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nitrogen['CharacteristicName'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nitrogen['ResultSampleFractionText'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subset columns\n",
    "usecols=[0,6,21,30,31,32,33,34,58,59,60]\n",
    "nitrogen = nitrogen.iloc[:,usecols]\n",
    "nitrogen.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection Limits and Unit Conversion\n",
    "You may have noticed that many sample sites state “not detected”. This is important data that are not currently being represented. Create a new column and set the value equal to the results, unless it is below the detection limit - in which case set it equal to ½ of the detection limit.\n",
    "\n",
    "You may also have noted that the total nitrogen was sometimes reported as mg/l or mg/l NO3. We want mg/l. To convert to mg/l, we know the atomic weight of nitrogen is 14.0067 and the molar mass of nitrate anion (NO3) is 62.0049 g/mole. Therefore, to convert between units:\n",
    "* Nitrate-N (mg/L) = 0.2259 x Nitrate-NO3 (mg/L)\n",
    "* Nitrate-NO3 (mg/L) = 4.4268 x Nitrate-N (mg/L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "♦ Set data below the detection limit equal to 1/2 the detection limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Set default values as equal to the ResultMeasureValue\n",
    "nitrogen['TotalN'] = nitrogen['ResultMeasureValue']\n",
    "\n",
    "#Step 2: Create a mask of values that are \"Not Detected\" and update those valuse\n",
    "ndMask = nitrogen['ResultDetectionConditionText'] == 'Not Detected'\n",
    "\n",
    "#Step 3: Apply the mask to select only the 'Not Detected' rows \n",
    "#        and calculate the TotalN values to be 1/2 the detection value\n",
    "nitrogen.loc[ndMask,\"TotalN\"] = nitrogen['DetectionQuantitationLimitMeasure/MeasureValue'] / 2\n",
    "\n",
    "#Finally: have a look at the mean Total N value\n",
    "nitrogen['TotalN'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "♦ Convert mg/l as NO3 to mg/l as N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a mask of rows measuring NO3\n",
    "no3Mask = nitrogen['DetectionQuantitationLimitMeasure/MeasureUnitCode'] == 'mg/l NO3'\n",
    "#Convert the TotalN values in those rows from NO3 to N\n",
    "nitrogen.loc[no3Mask,'TotalN'] = nitrogen['TotalN'] * 0.2259"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis & Visualizations\n",
    "Now that we have explored and tidied our `sites` and `results` datasets pulled from the Water Quality Data portal, we'll **merge** them together and generate our visualizations. Recalling our objective is to reveal trends in water quality, particularly in repsonse to water quality rules issued in 2009, we'll want to constrain (i.e. **filter**) our analyses to sites with data collected before and after 2009. It's these remaining sites that we'll construct **plots** of total nitrogen: line plots of N over time, box plots by year, and box plots by month. We'll also plot our concentrations on a **map**. \n",
    "\n",
    "Additionally, we'll convert our N concentrations to annual load (lbs/year), which will require pulling in discharge data (as we did in previous sessions). We'll do this for a few sites, comparing the findings to allowable levels to answer: ***are these sites in compliance with Total Daily Maximum Loads (TMDLs)?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge sites and measurements together\n",
    "Currently, site and measurement data are not connected together. However, we may want to show the nitrate values on a map. To do this, we merge the data together based on a unique identifier shared between the two data sets. In this case, it is the *MonitoringLocationIdentifier* column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge the two datasets, joining the sites to the results (as sites may have >1 result) \n",
    "nitrodata = pd.merge(left=sites,\n",
    "                     right=nitrogen,\n",
    "                     how='right',\n",
    "                     on='MonitoringLocationIdentifier')\n",
    "nitrodata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the `ActivityStartDate` from a string to an actual datetime object. Then we can extract year and month to new columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the ActivityStartDate to a datetime object (currently a string)\n",
    "nitrodata['ActivityStartDate'] = pd.to_datetime(nitrodata['ActivityStartDate'],format=\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create year values from the ActivityStartDate column\n",
    "nitrodata['Year'] = pd.DatetimeIndex(nitrodata['ActivityStartDate']).year\n",
    "nitrodata['Month'] = pd.DatetimeIndex(nitrodata['ActivityStartDate']).month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List the counts of records by month (easily changed to show year year)\n",
    "nitrodata['Month'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter merged data\n",
    "\n",
    "With the data merged, we now want to know how many sites are collecting data of interest. We find there are 22 sites (using the `nunique()` function below). Of those sites, we want to know which were collecting data after the Jordan Lake Rules were passed and how much data are being collected at this site.\n",
    "\n",
    "Based on this, we see several sites stopped collecting data prior to 2009, when the first iteration of Jordan Rules were passed. We also see that some of these sites collected only a few years of data. Remove those sites and plot the remaining sites on the map. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tally the number of unique sites\n",
    "nitrodata['MonitoringLocationIdentifier'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group data by site\n",
    "siteGroup = nitrodata.groupby('MonitoringLocationIdentifier')['Year']\n",
    "\n",
    "#Compute min and max years from our group\n",
    "siteInfo = siteGroup.agg(['min','max'])\n",
    "\n",
    "#Rename Colummns from min and max to StartYear and EndYear\n",
    "siteInfo.columns = ['StartYear','EndYear']\n",
    "\n",
    "#Compute Range\n",
    "siteInfo['Range'] = siteInfo['EndYear'] - siteInfo['StartYear']\n",
    "\n",
    "#Show our result\n",
    "siteInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Limit data to those with more than 10 years data and still operating\n",
    "siteInfo = siteInfo.query('Range >= 10 & EndYear >= 2017')\n",
    "siteInfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're down to 10 sites that fit our criteria! Now let's pull the water quality data for just those sites into a new dataframe called `dfSubSites` and then see where those sites fall on a map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter our merged sites-results dataframe to contain only the sites identified above\n",
    "siteMask = nitrodata['MonitoringLocationIdentifier'].isin(siteInfo.index)\n",
    "dfSubSites = nitrodata[siteMask]\n",
    "dfSubSites.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see there's 2366 water quality records for those 10 sites. Now we'll plot them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find center coordinates from medians of lat and long columns\n",
    "medLat = dfSubSites['LatitudeMeasure'].median()\n",
    "medLng = dfSubSites['LongitudeMeasure'].median()\n",
    "\n",
    "#Create the initial map\n",
    "m = folium.Map(location=[medLat,medLng],\n",
    "               zoom_start=10,\n",
    "               tiles='stamenterrain')\n",
    "\n",
    "#Group and map these sites; this will speed mapping by removing duplicate locations\n",
    "dfSubSites2 = dfSubSites.groupby('MonitoringLocationIdentifier').first()\n",
    "\n",
    "#Loop through all features and add them to the map as markers\n",
    "for row in dfSubSites2.itertuples():\n",
    "    #Get info for the record\n",
    "    lat = row.LatitudeMeasure\n",
    "    lng = row.LongitudeMeasure\n",
    "    name = row.MonitoringLocationName\n",
    "    #Create the marker object, adding them to the map object\n",
    "    folium.CircleMarker(location=[lat,lng],\n",
    "                        popup=name,\n",
    "                        color='red',\n",
    "                        fill=True,\n",
    "                        fill_opacity=0.8,\n",
    "                        radius=5,\n",
    "                        stroke=False).add_to(m)\n",
    "    \n",
    "#Show map\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot total nitrogen over time, by year and by month, for each sites\n",
    "Now that we've seen what sites were dealing with, let's move on to create some plots of these sites. For these 10 sites, we want to display:\n",
    "* Line plots of total N across the site's period of record.\n",
    "* Box plots of total N grouped by year and by month, to view trends within and across groups.\n",
    "\n",
    "This get's a bit convoluted but the approach is as follows:\n",
    "* Create a list of the unique siteIDs in the table\n",
    "* Iterate through each site; for each site\n",
    " * Construct a figure template consisting of 1 row and 3 columns (a total of 3 axes)\n",
    " * Create a line plot of TotalN over time and put this in the 1st row, 1st column\n",
    " * Create a box plot of TotalN by *month* and put this in the 1st row, 2nd column\n",
    " * Create a box plot of TotalN by *year* and put this in the 1st row, 3nd column\n",
    " * Display the plot \n",
    " * Move to the next site, adding more 1x3 plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create list of siteIDs\n",
    "siteIDs = dfSubSites['MonitoringLocationIdentifier'].unique().tolist()\n",
    "siteIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iterate through each siteID\n",
    "for i,siteID in enumerate(siteIDs):\n",
    "    \n",
    "    #Create the figure for the current site\n",
    "    fig, ax = plt.subplots(1,3)\n",
    "    fig.set_figheight(3)\n",
    "    fig.set_figwidth(15)\n",
    "\n",
    "    #Subset the data for the given site\n",
    "    siteMask = dfSubSites['MonitoringLocationIdentifier'] == siteID\n",
    "    dfSite = dfSubSites[siteMask].sort_index()\n",
    "    \n",
    "    #Create sub plots\n",
    "    dfSite['TotalN'].plot(title=siteID,ax=ax[0])\n",
    "    dfSite.boxplot(column='TotalN',by='Month',rot=90,ax=ax[1])\n",
    "    dfSite.boxplot(column='TotalN',by='Year',rot=90,ax=ax[2])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the 2017 Nitrogen year on leaflet\n",
    "Here we plot the 10 sites on a map, but this time we size the markers to reveal mean total nitrogen for year 2017 records..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select records for 2017\n",
    "df2017 = dfSubSites[dfSubSites.Year == 2017]\n",
    "\n",
    "#Group by site & compute mean TotalN\n",
    "df2017sites = df2017.groupby('MonitoringLocationIdentifier').mean()\n",
    "df2017sites.TotalN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find center coordinates from medians of lat and long columns\n",
    "medLat = df2017sites['LatitudeMeasure'].median()\n",
    "medLng = df2017sites['LongitudeMeasure'].median()\n",
    "\n",
    "#Create the initial map\n",
    "m = folium.Map(location=[medLat,medLng],\n",
    "               zoom_start=10,\n",
    "               tiles='stamenterrain')\n",
    "\n",
    "#Loop through all features and add them to the map as markers\n",
    "for row in df2017sites.itertuples():\n",
    "    #Get info for the record\n",
    "    lat = row.LatitudeMeasure\n",
    "    lng = row.LongitudeMeasure\n",
    "    avgN = row.TotalN\n",
    "    #Create the marker object, adding them to the map object\n",
    "    folium.CircleMarker(location=[lat,lng],\n",
    "                        popup=str(avgN),\n",
    "                        color='red',\n",
    "                        fill=True,\n",
    "                        fill_opacity=0.8,\n",
    "                        radius=5*avgN,  #<-- we set the marker radius with avgN\n",
    "                        stroke=False).add_to(m)\n",
    "    \n",
    "#Show map\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the Nitrogen Load with Thresholds\n",
    "The water quality data reports Nitrogen as $mg/l$. In order to convert to an annual load ($lbs/yr$), we need to know the volume of water flowing through each site. You could go to the NWIS Mapper to find which USGS gauges are closest to the Haw River arm (1 site) and the New Hope arm (3 sites), but we'll give them to you here: Haw River arm `02096960`; Hope River arm: {`02097314`,`02097517`,`0209741955`}\n",
    "\n",
    "Now our task is to extract the data for these sites, for which we'll create an apply a Python function called `getNWISData`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load NWIS data for Haw River"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x  = list(range(26))#.append(27)\n",
    "x.append(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to retrieve discharge data for the specified site\n",
    "def getNWISData(siteNo):\n",
    "    #import libraries\n",
    "    import requests, io\n",
    "    \n",
    "    #Construct the service URL and parameters\n",
    "    url =  'https://waterservices.usgs.gov/nwis/dv'\n",
    "    params = {'sites':siteNo,        # <--The site number provided\n",
    "              'parameterCd':'00060', # <--Discharge\n",
    "              'statCd':'00003',      # <--Mean\n",
    "              'startDT':'1930-10-01',# <--Start date\n",
    "              'endDT':'2017-12-31',  # <--End date\n",
    "              'format':'rdb',        # <--Output as table\n",
    "              'siteStatus':'all'     # <--Get all sites (active and inactive)\n",
    "             }\n",
    "    response_raw =  requests.get(url,params)\n",
    "    response_clean =response_raw.content.decode('utf-8')\n",
    "    \n",
    "    #**Convert the data into a data frame**\n",
    "    \n",
    "    #Build a list of line numbers to skip from comments and data types\n",
    "    rowsToSkip= [] # <--Create an empty list to hold line numbers\n",
    "    lineNumber = 0 # <--Initialize a line number variable\n",
    "    \n",
    "    #Iterate through lines, adding the line number to the list for comment lines\n",
    "    for lineNumber, lineString in enumerate(response_clean.split(\"\\n\")):\n",
    "        if lineString.startswith('#'): \n",
    "            rowsToSkip.append(lineNumber)\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    #Add another line 2 greater than the last\n",
    "    dataTypeLineNumber = rowsToSkip[-1] + 2\n",
    "    rowsToSkip.append(dataTypeLineNumber)\n",
    "    \n",
    "    #Create a dataframe from the downloaded data\n",
    "    dfSite = pd.read_csv(io.StringIO(response_clean),\n",
    "                         skiprows=rowsToSkip,\n",
    "                         delimiter='\\t',\n",
    "                         dtype={'site_no':'str'})\n",
    "\n",
    "    #Convert datatype for datetime\n",
    "    dfSite['datetime'] = pd.to_datetime(dfSite['datetime'])\n",
    "    \n",
    "    #Rename the columns\n",
    "    dfSite.columns = ['agency_cd','site_no','datetime','meanflow_cfs','confidence']\n",
    "    \n",
    "    #Add year and month columns\n",
    "    dfSite['Year'] = dfSite['datetime'].map(lambda x: x.year)\n",
    "    dfSite['Month'] = dfSite['datetime'].map(lambda x: x.month)\n",
    "    \n",
    "    #Set the index as the datetime column\n",
    "    dfSite.index= dfSite.datetime\n",
    "\n",
    "    #Fix any errors\n",
    "    dfSite['meanflow_cfs'] = pd.to_numeric(dfSite['meanflow_cfs'],errors='coerce')\n",
    "    \n",
    "    return dfSite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get site info for the specified site\n",
    "def getNWISSiteData(siteNo):\n",
    "    url=('https://waterdata.usgs.gov/nwis/inventory?'+\\\n",
    "         'search_site_no={}'.format(siteNo)+\\\n",
    "         '&search_site_no_match_type=exact'+\\\n",
    "         '&group_key=NONE'+\\\n",
    "         '&format=sitefile_output'+\\\n",
    "         '&sitefile_output_format=rdb'+\\\n",
    "         '&column_name=dec_lat_va'+\\\n",
    "         '&column_name=dec_long_va'+\\\n",
    "         '&column_name=drain_area_va'+\\\n",
    "         '&list_of_search_criteria=search_site_no')\n",
    "    dfSiteX = pd.read_csv(url,skiprows=range(26),\n",
    "                          sep='\\t',\n",
    "                          names=['lat','lng','agcy','datum','d_area'])\n",
    "    dfSiteX.index = [siteNo]\n",
    "    return dfSiteX[['lat','lng','d_area']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the NWIS Discharge data for the Haw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get data for the Haw River (siteNo = '02096960') using the function above\n",
    "dfHaw_Discharge = getNWISData('02096960')\n",
    "dfHaw_Discharge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the site info for the Haw River site\n",
    "dfHaw_SiteInfo = getNWISSiteData('02096960')\n",
    "dfHaw_SiteInfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the Water Quality data for the Haw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter site data for the Haw River location\n",
    "dfHawWQ = dfSubSites[dfSubSites['MonitoringLocationIdentifier'] == 'USGS-0209699999']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Annual Load for Haw River\n",
    "\n",
    "To calculate the annual load we need to convert cfs to MGD. Then we use pipes and dplyr to calculate the total annual flow at Haw River. Next, we calculate the average Nitrogen load for samples taken during each year. This is a rough proxy. A finer analysis can be undertaken by summarizing monthly flow and water quality, aggregating to the year as the last step. The annual load is then the: Total Flow * average Nitrogen * 8.34 lbs per gallon\n",
    "\n",
    "We can then plot the annual load with the threshold of 2.567 Million pounds per year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Convert cms to MGD in the discharge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the MGD column to the dfHaw table\n",
    "dfHaw_Discharge['flow_MGD'] = dfHaw_Discharge['meanflow_cfs'] * 0.64631688969744"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compute annual flow from NWIS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group data by year and compute the total flow and count of records\n",
    "dfAnnualMGD = dfHaw_Discharge.groupby('Year')['flow_MGD'].agg(['sum','count'])\n",
    "\n",
    "#Remove records with fewer than 350 records\n",
    "dfAnnualMGD = dfAnnualMGD[dfAnnualMGD['count'] > 350]\n",
    "\n",
    "#Rename the 'sum' column\n",
    "dfAnnualMGD.columns = ['AnnualFlow_MGD','Count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compute mean concentration from WQ data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute annual Nitrate concentration\n",
    "wqYear = dfHawWQ.groupby('Year')['TotalN'].mean()\n",
    "dfAnnualN = pd.DataFrame(wqYear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Join the totalN and annual discharge tables on their index (year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haw = pd.merge(left=dfAnnualN,right=dfAnnualMGD,how='inner',left_index=True,right_index=True)\n",
    "haw.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compute pounds per day (`MGD` \\* `avg N` * `8.34 lbs/gal`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haw['M_lbs'] = haw[\"AnnualFlow_MGD\"] * haw['TotalN'] * 8.34 / 1000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot the data. Here we use the matplotlib \"object oriented\" format which allows us to format more things. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct the figure object\n",
    "fig = plt.figure()\n",
    "\n",
    "#Create an axis object, which is what we plot on\n",
    "ax = plt.axes()\n",
    "\n",
    "#Create a line plot of M_lbs against our index (years), set markers to be points\n",
    "plt.plot(haw['M_lbs'],marker='o')\n",
    "\n",
    "#Add aesthetics\n",
    "plt.title(\"Haw River Branch\")\n",
    "plt.ylabel(\"Annual Nitrogen Load (million lbs)\")\n",
    "\n",
    "#Add a line at the TMLD limit, color it in red an make it dashed\n",
    "plt.axhline(y=2.567,color='red',linestyle='--')\n",
    "\n",
    "#Finally, reveal the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write out data to csv\n",
    "haw.to_csv('./haw_python.csv',index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Annual Load for New Hope Creek arms\n",
    "\n",
    "Here there are 3 upstream gauges for New Hope Creek. We will download that information into a single file. We repeat the above analyis with the New Hope stream gauges. The three gage site IDs are `02097314`, `02097517`, and `0209741955`. Use the Total N values from this WQ site `USGS-0209768310`. \n",
    "\n",
    "* Use the `getNWISData()` function already created to pull NWIS data for each site\n",
    "* Extract the \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the data\n",
    "dfQ1 = getNWISData('02097314')\n",
    "dfQ2 = getNWISData('02097517')\n",
    "dfQ3 = getNWISData('0209741955')\n",
    "\n",
    "dfS1 = getNWISSiteData('02096960')\n",
    "dfS2 = getNWISSiteData('02097517')\n",
    "dfS3 = getNWISSiteData('0209741955')\n",
    "\n",
    "dfWQ = dfSubSites[dfSubSites.MonitoringLocationIdentifier == 'USGS-0209768310']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add MGD columns\n",
    "dfQ1['flow_MGD'] = dfQ1['meanflow_cfs'] * 0.64631688969744\n",
    "dfQ2['flow_MGD'] = dfQ2['meanflow_cfs'] * 0.64631688969744\n",
    "dfQ3['flow_MGD'] = dfQ3['meanflow_cfs'] * 0.64631688969744"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute annual flow for each dataset\n",
    "dfQ1_annual = dfQ1.groupby('Year').agg({'flow_MGD':('sum','count')})\n",
    "dfQ1_annual.columns = dfQ1_annual.columns.droplevel()\n",
    "dfQ1_annual = dfQ1_annual[dfQ1_annual['count'] > 350]\n",
    "dfQ1_annual.columns = ['AnnualFlow_MGD','Count']\n",
    "\n",
    "dfQ2_annual = dfQ2.groupby('Year').agg({'flow_MGD':('sum','count')})\n",
    "dfQ2_annual.columns = dfQ2_annual.columns.droplevel()\n",
    "dfQ2_annual = dfQ2_annual[dfQ2_annual['count'] > 350]\n",
    "dfQ2_annual.columns = ['AnnualFlow_MGD','Count']\n",
    "\n",
    "dfQ3_annual = dfQ3.groupby('Year').agg({'flow_MGD':('sum','count')})\n",
    "dfQ3_annual.columns = dfQ3_annual.columns.droplevel()\n",
    "dfQ3_annual = dfQ3_annual[dfQ3_annual['count'] > 350]\n",
    "dfQ3_annual.columns = ['AnnualFlow_MGD','Count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute annual Nitrate concentration\n",
    "wqYear = dfWQ.groupby('Year')['TotalN'].mean()\n",
    "dfWQ_annual = pd.DataFrame(wqYear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join the tables\n",
    "df1 = pd.merge(left=dfWQ_annual,right=dfQ1_annual,how='inner',left_index=True,right_index=True)\n",
    "df2 = pd.merge(left=dfWQ_annual,right=dfQ2_annual,how='inner',left_index=True,right_index=True)\n",
    "df3 = pd.merge(left=dfWQ_annual,right=dfQ3_annual,how='inner',left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute 1000 lbs/day from MGD and TotalN\n",
    "df1['K_lbs-Site1'] = df1[\"AnnualFlow_MGD\"] * df1['TotalN'] * 8.34 / 100000\n",
    "df2['K_lbs-Site2'] = df2[\"AnnualFlow_MGD\"] * df2['TotalN'] * 8.34 / 100000\n",
    "df3['K_lbs-Site3'] = df3[\"AnnualFlow_MGD\"] * df3['TotalN'] * 8.34 / 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the data\n",
    "fig = plt.figure(figsize=(20,8))\n",
    "ax = plt.axes()\n",
    "\n",
    "plt.plot(df1['K_lbs-Site1'],marker='o',color='black')\n",
    "plt.plot(df2['K_lbs-Site2'],marker='o',color='green')\n",
    "plt.plot(df3['K_lbs-Site3'],marker='o',color='blue')\n",
    "plt.title(\"New Hope Reaches\")\n",
    "plt.ylabel(\"Annual Nitrogen Load (million lbs)\")\n",
    "plt.axhline(y=2.567,color='red',linestyle='--')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
